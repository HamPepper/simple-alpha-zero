<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project
  | Georgia Institute of Technology | Fall 2018: CS 4803 / 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>AlphaZero from Scratch</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Nick Petosa, Robert Keezer, Nupur Chatterji </strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4803 / 7643 Deep Learning: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Institute of Technology</span>
<hr>

<!-- Goal -->
<h2>Abstract</h2>
<!-- 
One or two sentences on the motivation behind the problem you are solving. 
One or two sentences describing the approach you took. 
One or two sentences on the main result you obtained. -->

<p> We were inspired by DeepMind's AlphaZero - a general algorithm that can learn to master a game just from its rules, without any prior knowledge or expert-played games as training data. The AlphaZero algorithm uses a combination of deep neural networks and tree search and is a generalization of the AlphaGo Zero algorithm (used specifically for the game of Go). </p>

<p> AlphaZero leverages Monte Carlo Tree Search (MCTS) and a deep learning heuristic in order to traverse through various promising game states.
  For our project, we implemented AlphaZero completely from scratch, and then tested it on small games such as Tic-Tac-Toe and Connect 4. 
  
<p>Additionally, we experiment using a state of the art squeeze-and-excitation network as our architecture, which has not previously been tried in AlphaZero literature. </p>

<p> TO DO: MAIN RESULTS. </p>

<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<!-- SOURCE: https://www.google.com/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwj95M6DsvDeAhXpQd8KHWFCDv0QjRx6BAgBEAU&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMonte_Carlo_tree_search&psig=AOvVaw0x_Q2shAPS29D3Pj6xeL0A&ust=1543264316465916 -->
<img style="height: 200px;" alt="" src="images/mcts.png">
</div>
<br><br>

<h2> Introduction and Motivation </h2>
<h4>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</h4>
<p>Our goal was to independently replicate the implementation of AlphaZero and test it out on games such as Tic-Tac-Toe and Connect-4. Since the algorithm is generalized, the same algorithm can be plugged into any discrete, deterministic game with perfect information, and should be able to find powerful policies through self-play. Just like the results of the original paper, we wanted to show that the algorithm would be able to learn how to play (and win!) the game based on solely its associated rules. We would not provide the algorithm with any other information such as game trajectories by grandmasters etc.</p>

<h4>How is it done today, and what are the limits of current practice?</h4>
<p>Currently, AlphaZero is at the forefront of systems that can learn by itself (i.e. through self-play) and master highly complex games (such as Chess and Go). Stockfish has also been known to dabble in this field, but their results have not been as impressive as AlphaZero. In fact, in a match of 100 games, AlphaZero beat Stockfish 64-36, a landslide victory. We do note though that there have been doubts about the validity of these claims. AlphaZero's key benefit lies in the fact that it does not require masses of data and can self-learn how to play the game. Combined with the increase in computational power now available, training the network to play a game of choice has almost become straightforward. Nobody has yet been able to replicate the results that DeepMind achieved. </p>

<p> Connect 4 and tictactoe being solved; different searches (Alpha Beta, Minimax) check paper; we cannot replicate because TPU not available; </p>

<h4>Who cares? If you are successful, what difference will it make?</h4>
<p> As mentioned before, our goal was to implement the AlphaZero algorithm from scratch. We believe that the implementation is far more accessible and modular, and it is well-tested. In the code bank, there is a test bed that can be scaled up (perhaps using TPUs). Having this set up will also provide good headway for future research - testing different types of deep nets, introducing multi-player etc.</p>

<p> Modular (swap out the game and swap out the network) </p>

<p> TO DO: Multi-Player? Talk about the SENet etc.? </p>

<br><br>
<h2> Background: Monte Carlo Tree Search (MCTS)</h2>
<p> The AlphaZero algorithm uses MCTS for policy improvement and competetive play. AlphaZero improves upon basic MCTS by introducing a deep heuristic during search. We first wanted to understand how basic, uninformed MCTS works. OVERVIEW SENTENCE.

The algorithm works with games that are discrete, deterministic and have perfect information - this means that they must have distinct moves and positions, have fixed outcomes for each move, players must compete against one another but can see everything that happens. </p>

<p> The search tree always begins by expanding the game's starting state s<sub>init</sub>, by considering every possible action. For a game like Tic-Tac-Toe this initial state would be an empty 3 by 3 board, and a possible action would be placing an X in the center slot. The diagram below shows how this first step works for a state further down the Tic-Tac-Toe game tree. </p>

<br><br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="images/mcts1.png">
</div>
<br><br>

<p> Each of these new child nodes is then played out by making random moves until a final state (winning, losing or tying) is reached. These finals states have associated scores - wins get +1, ties get 0 and losses get -1. Reaching an outcome (win/loss/tie) through random play gives us an indication as to how promising a state is. One example of playing out a child state is shown below. </p>

<br><br>
<div style="text-align: center;">
<img style="height: 100px;" alt="" src="images/mcts2.png">
</div>
<br><br>

TO DO: edge, re do diagrams

<p> The value of the node (W) and the number of play outs that have been executed (N) is stored for each edge. These values are then sent up through the tree and contribute to the W and N of the parent node. The accumulated values are stored for the parent node. These steps are shown below.</p>

<br><br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="images/mcts3.png">
</div>
<br><br>

<div style="text-align: center;">
<img style="height: 350px;" alt="" src="images/mcts4.png">
</div>
<br><br>

<p> This combination of expanding nodes, unravelling their value and the propagating the information upwards occurs over multiple iterations. The algorithm does not expand all the leaf nodes (this would be very computationally expensive), and instead it chooses nodes that balance between having high estimated values (W) and having low visit counts (N). MCTS chooses to expand the leaf with the highest upper confidence tree score (UCT) - the equation is shown below. </p>

<div style="text-align: center;">
<img style="height: 100px;" alt="" src="images/mcts5.png">
</div>
<br><br>

<p> Having a low value for c wil encourage the algorithm to choose nodes with a high W, while a low c value will encourage the search to pick nodes with a low N to explore. The value of c is often set empirically. The W<sub>i</sub> and N<sub>i</sub> correspond to the accumulated value and the visit count for the i<sup>th</sup> child. N<sub>p</sub> is the visit count of the parent node. At c = 1, the UCT score's for the Tic-Tac-Toe tree we have been following are shown below: </p>

<div style="text-align: center;">
<img style="height: 400px;" alt="" src="images/mcts6.png">
</div>
<br><br>

<p> Since state s<sub>0, 1</sub> has the highest UCT score, it is picked first to explore. As before, it is expanded, and the values are propagated back through the tree. In this example, the W value reflects player X's wins and losses. The algorithm keeps track of the current player (and flips the sign of W whenever the players switch). </p>

<div style="text-align: center;">
<img style="height: 500px;" alt="" src="images/mcts7.png">
</div>
<br><br>

<p> The MCTS will continue to run multiple iterations until time runs out. Based on the chosen and tuned parameters, the tree goes through a good balance of expansion and exploration to eventually identify the best move to make. Then in the original game, the first child with the highest N value is picked. If the final tree looked like the one below, the algorithm would choose the action that led to s<sub>0, 1</sub>. </p>

<div style="text-align: center;">
<img style="height: 350px;" alt="" src="images/mcts8.png">
</div>
<br><br>

<h2>Approach</h2>
<h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4>

We implemented AlphaZero from scratch and used it train an agent to master Tic-Tac-Toe and Connect 4. Our implementation is done using python, numpy, and pytorch.
We believed that we would be able to succeed because both games have a relatively small branching factor and number of game states compared to a game like Go.
and the AlphaZero algorithms purports to be generalizable to different games. We directly compare the performance of the algorithm across three different network architectures.<br>
TO DO: Mention multi-player support.

<h4>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</h4>

One problem we anticipated was computation time. From the AlphaZero paper, training an agent to play Go took just 8 hours to master the game - but required over 5000 TPUs.
The only resource we had access to was a GTX 1070 in a desktop machine running Ubuntu.
It would take approximately 22 years to replicate DeepMind's Go program on our hardware.
It was important to limit the scope of this project to something feasible with our hardware by picking a simpler game.
Training a Tic-Tac-Toe agent was our immediate goal, and training a Connect 4 agent was our reach goal.
Go has has an average branching factor 250 and about 2*10<sup>170</sup> game states.
Compare that to the simplicity of the other games:
Tic-Tac-Toe has a maximum branching factor of 9 and about 5000 game states, and Connect 4 has a maximum branching factor of 6 and about 4.5 trillion game states.
<br><br>
Another problem we anticipated was the complexity of implementing AlphaZero from scratch.
There are many intricate components like MCTS that are bug-prone.
If any component was bugged, the entire AlphaZero pipeline would fail.
To couteract this, we wrote extensive unit tests for all components during development.
In total, we created 31 tests for our implementation.
<br><br>
An unanticipated problem that we encountered was AlphaZero's sensitivity to certain hyperparameters.

AlphaZero works by generating games of self-play as training data, and then updating its network based off that experience.
One tricky hyperparameter we had to tune was the number of self-play examples generated per network.
The AlphaZero paper offered no guidance on this value, so we initially started it at 1 - that is, 1 game generated per network.
We found that at this frequency learning was unstable - policies would change drastically between subsequent networks, impairing learning.
After some experimenting, we found that generating at least 30 training data games per iteration enabled stable learning.

<h4>Hyperparameters</h4>
One of the attractive qualities of the AlphaZero paper is that DeepMind used the same hyperparameters across each game they trained it on.
Similarly, after much trial and error, we arrived at a set of hyperparameters that worked well across different games and networks.
Experimenting with different hyperparameters is expensive because AlphaZero takes a long time to train, so once we discovered a set of hyperparameters that yielded good performance across many games and networks we stopped tweaking.

<br><br>
Algorithm parameters
<ul>
  <li>Number of self-play games generated per network: 30</li>
  <li>Number of batches per update: 1000</li>
  <li>MCTS iterations per move: 50 (Set as 800 in AlphaZero paper)</li>
  <li>cpuct: 3</li>
</ul>
<br>
Network parameters
<ul>
  <li>Optimizer: Adam (SGD+Momentum in AlphaZero paper)</li>
  <li>Learning rate: 1e-3 (1e-2 -> 1e-4 decay in AlphaZero paper)</li>
  <li>Batch size: 64 (2048 in AlphaZero paper)</li>
  <li>L2 regularization: 1e-4 (Same as AlphaZero paper)</li>
</ul>

To avoid overfitting, L2 regularization is set to 1e-4 as suggested in the AlphaZero paper.
Generalization is important for this task because the network will only see a small subset of all possible boards during training.
Generalization empowers the network to have a good sense as to whether a novel state encountered during play is promising, which is useful if the game enters unknown territory.

<br><br>
<h2>Experiments and Results</h2>
<h4>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4>
<p>
Since we experimented with two separate games, we will report the results for both games separately. 
Overall, our implementation of AlphaZero was successful with both games.
We experimented with three different network architectures, described below.
</p>
<table border="1" align="center" style="margin-top: 30px; margin-bottom: 30px;">
  <tr>
    <th>Name</th> <th>Description</th> <th>Number of parameters</th>
  </tr>
  <tr>
    <td>MLP</td> <td>Two-layer fully connected network with 200 hidden units.</td> <td>7,610</td>
  </tr>
  <tr>
      <td>Mini/Small VGG</td> <td>Two-layer convolutional network with 2x2 or 4x4 filters.</td> <td>~40,000</td>
  </tr>
  <tr>
      <td>SENet</td> <td>A deeper (4-layer) squeeze-and-excitation convolutional network.</td> <td>11,260,354</td>
  </tr>
</table>
<p>
These models are quite anemic compared to the 40-layer ResNet used by DeepMind, but our hope was that simpler games could be learned with simpler models.
</p>
<h5> Tic-Tac-Toe </h5>
<p>
  One measure of success is how well the network learned from self-play data.
  We can analyze the training loss curve of our networks.
  The lower the loss achieved, the more successful of a heuristic was learned.
  Training error only tells us how successful our network is during self-play - it does not necessarily generalize to real-world games.
  That said, assuming proper regularization, lower training error means a smarter agent.
</p>
<img src="images/ttt_error.png"/>
<p>
  Regardless of architecture choice, with more and more iterations of training, train error decreases.
  This result on its own is exciting; deep RL techniques are notoriously unstable, yet looking at this graph we see stable learning.
  Network architecture has a noticeable impact on error - the two layer fully-connected MLP has higher error after the same number of iterations as the shallow CNN (MiniVGG) or deep CNN (SENet).
  For a simple game like Tic-Tac-Toe, making our network deeper does not confer a significant advantage over using a shallower network.
</p>

<p>
  So the convolutional networks have lower self-play training error, but we would like to know how well they generalize.
  To test the general strength of each network, we developed a novel benchmark: each agent plays against vanilla MCTS agents of increasing strength and records its win rate.
  Vanilla MCTS agents do not use a deep heuristic; they use the UCT heuristic discussed earlier.
  After each turn, the number of MCTS iterations used by the opponent doubles, starting from 10 and ending at 20,480.
  In this way, we can compare the relative strengths of the networks against an identical opponent to get a better idea of how well their strategies generalize outside of self-play.
  The performance of these networks against increasingly stronger opponents is plotted below.
</p>
<img src="images/ttt_strength.png"/>


<p>  
  Just like before, the convolutional networks out-perform the MLP network.
  The convolutional networks have a higher win rate against general opponents,
  suggesting that they learned a stronger heuristic than the fully connected network.
  </p>


<p>
  As you might expect, as opponent strength increases, AlphaZero's win rate starts to decrease.
  Win rate converges to 50, which makes sense since optimal Tic-Tac-Toe play is known to end in a tie.
  All of our networks seemed to learn how to force a tie, though the convolutional networks also learned how to exploit weaker opponents and pull off more wins.
</p>

<p>
  Recall from our hyperparameter discussion that our trained AlphaZero agents use 50 MCTS iterations per turn, guided by a deep heuristic.
  For comparison, we have also plotted a 50-iteration vanilla MCTS agent as a control in black. 
  At each opponent, our networks meet or beat the win rate of the control, showing that the learned heuristics are meaningful.
</p>
  
<p>
  From these results, we see that our deep heuristic makes MCTS wildly more efficient compared to the "dumb" Vanilla MCTS UCT heuristic.
</p>

  <hr>
  <footer> 
  <p>This webpage template is based on a similar one from Dr. Devi Parikh's
  <a href="https://samyak-268.github.io/F18CS4476/">Intro to Computer Vision course</a>.</p>
  <p> The MCTS diagrams were based on a <a href="http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/"> blog </a></p>
  <p>© Nick Petosa, Robert Keezer, Nupur Chatterji </p>
  </footer>
</div>
</div>

<br><br>

</body></html>
